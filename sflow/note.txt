[root@dev-r630-03 ~]# ovs-vsctl list sflow
_uuid               : 1a79913e-670d-4cf8-982e-e4e96621cc25
agent               : "eno1"
external_ids        : {}
header              : 128
polling             : 10
sampling            : 10
targets             : ["10.75.205.14:8087"]

tcpdump -ni eno1 udp port 8087

[root@dev-r630-03 ~]# sudo ovs-vsctl -- --id=@sflow create sflow agent=eno1 target=\"10.75.205.14:8087\" header=128 sampling=10 polling=10 -- set bridge br sflow=@sflow
1a79913e-670d-4cf8-982e-e4e96621cc25

ovs-vsctl -- clear Bridge br sflow

sudo ovs-vsctl -- --id=@sflow create sflow agent=eno1 target=\"10.75.205.13:8087\" header=128 sampling=10 polling=10 -- set bridge br sflow=@sflow

ovs-vsctl -- --id=@sflow create sflow agent=eno1 target=\"10.75.205.14:6343\" header=128 sampling=10 polling=10 -- set bridge br sflow=@sflow


crash> rd -8 0xffff8c4d81cac29c 0x54
ffff8c4d81cac29c:  54 00 06 00 0c 00 04 00 01 58 25 93 99 99 99 19   T........X%.....
ffff8c4d81cac2ac:  44 00 02 00 08 00 01 00 09 d1 07 aa 34 00 02 00   D...........4...
ffff8c4d81cac2bc:  01 00 00 00 03 00 00 00 02 f2 cd 6b e5 4c f6 99   ...........k.L..
ffff8c4d81cac2cc:  30 38 63 bf 2e 63 eb d1 00 00 00 00 48 00 00 00   08c..c......H...
ffff8c4d81cac2dc:  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   ................
ffff8c4d81cac2ec:  04 00 04 00                                       ....
crash>


enum ovs_userspace_attr {
        OVS_USERSPACE_ATTR_UNSPEC,
1        OVS_USERSPACE_ATTR_PID,       /* u32 Netlink PID to receive upcalls. */
2        OVS_USERSPACE_ATTR_USERDATA,  /* Optional user-specified cookie. */
3        OVS_USERSPACE_ATTR_EGRESS_TUN_PORT,  /* Optional, u32 output port
                                              * to get tunnel info. */
4        OVS_USERSPACE_ATTR_ACTIONS,   /* Optional flag to get actions. */
        __OVS_USERSPACE_ATTR_MAX
};

         0x54    0xc                 0x44
crash> # actions:sample(sample=10.0%,actions(      userspace(pid=2852638985,  sFlow(vid=0,pcp=0,output=72),  actions)    )),enp4s0f0_1

/* user_action_cookie is passed as argument to OVS_ACTION_ATTR_USERSPACE. */
struct user_action_cookie {
    uint16_t type;              /* enum user_action_cookie_type. */
    ofp_port_t ofp_in_port;     /* OpenFlow in port, or OFPP_NONE. */
    struct uuid ofproto_uuid;   /* UUID of ofproto-dpif. */

    union {
        struct {
            /* USER_ACTION_COOKIE_SFLOW. */
            ovs_be16 vlan_tci;      /* Destination VLAN TCI. */
            uint32_t output;        /* SFL_FLOW_SAMPLE_TYPE 'output' value. */
        } sflow;
    };
};


/* The first action is always 'OVS_SAMPLE_ATTR_ARG'. */

                          OVS_ACTION_ATTR_SAMPLE = 6
                                             OVS_SAMPLE_ATTR_PROBABILITY = 1 (58 25 93 are random number for padding)
ffff8c4d81cac29c:  (54 00 06 00 (0c 00 04 00 01 58 25 93 99 99 99 19)   T........X%.....
                                       OVS_SAMPLE_ATTR_ARG

0x19999999 = 429496729 = 419430.40K = 409.60M = 0.40G

[root@dev-r630-04 ~]# qalc
> 0x19999999/0xffffffff

  429496729 / 4294967295 = approx. 0.1

means sampling=10 = %10

                                       OVS_USERSPACE_ATTR_PID (aa07d109)  
ffff8c4d81cac2ac:  (44 00 02 00 (08 00 01 00 09 d1 07 aa)           (34 00 02 00   D...........4...
                          OVS_ACTION_ATTR_USERSPACE                        OVS_ACTION_ATTR_USERSPACE ?

                               port=3      uuid 16 bytes
ffff8c4d81cac2bc:  01 00 00 00 03 00 00 00 02 f2 cd 6b e5 4c f6 99   ...........k.L..
                   USER_ACTION_COOKIE_SFLOW

                                           vlan_tci
ffff8c4d81cac2cc:  30 38 63 bf 2e 63 eb d1 00 00 00 00 48 00 00 00   08c..c......H...
                                                       output=72


ffff8c4d81cac2dc:  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00)   ................

ffff8c4d81cac2ec:  04 00 04 00))
                         OVS_USERSPACE_ATTR_ACTIONS




crash> rd -8 0xffff8c4d7c6fd71c 0x5c
ffff8c4d7c6fd71c:  54 00 06 00 0c 00 04 00 01 58 25 93 99 99 99 19   T........X%.....
ffff8c4d7c6fd72c:  44 00 02 00 08 00 01 00 83 2d 23 d0 34 00 02 00   D........-#.4...
ffff8c4d7c6fd73c:  01 00 00 00 02 00 00 00 02 f2 cd 6b e5 4c f6 99   ...........k.L..
ffff8c4d7c6fd74c:  30 38 63 bf 2e 63 eb d1 00 00 00 00 49 00 00 00   08c..c......I...
ffff8c4d7c6fd75c:  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00   ................
ffff8c4d7c6fd76c:  04 00 04 00 08 00 01 00 03 00 00 00               ............



a[root@dev-r630-04 ~]# trace tcf_sample_init -K
TIME     PID     TID     COMM            FUNC
5.173285 4273    4273    tc              tcf_sample_init
        tcf_sample_init+0x1 [act_sample]
        tcf_action_init+0x115 [kernel]
        tcf_exts_validate+0xcb [kernel]
        mall_change+0x168 [cls_matchall]
        tc_new_tfilter+0x861 [kernel]
        rtnetlink_rcv_msg+0x369 [kernel]
        netlink_rcv_skb+0x50 [kernel]
        rtnetlink_rcv+0x15 [kernel]
        netlink_unicast+0x1a8 [kernel]
        netlink_sendmsg+0x233 [kernel]
        sock_sendmsg+0x65 [kernel]
        ____sys_sendmsg+0x215 [kernel]
        ___sys_sendmsg+0x81 [kernel]
        __sys_sendmsg+0x5c [kernel]
        __x64_sys_sendmsg+0x1f [kernel]
        do_syscall_64+0x5a [kernel]
        entry_SYSCALL_64_after_hwframe+0x44 [kernel]


===ct===

nf_flow_offload_init init flow_cls_offload.cookie

CONFIG_NET_TC_SKB_EXT
mlx5e_tc_rep_update_skb


[root@dev-r630-04 kernel]# ./flowtables.py
hostname: dev-r630-04

nf_flowtable ffff8bf33c2dac70
nf_ft.gc_work.work.func: nf_flow_offload_work_gc
        cb: mlx5_tc_ct_block_flow_offload
        mlx5_ct_ft ffff8bf2c61d4e00



3.341207 21000   21000   kworker/0:0     mlx5_fc_query_cached
        mlx5_fc_query_cached+0x1 [mlx5_core]
        nf_flow_offload_tuple.isra.0+0xd4 [nf_flow_table]
        flow_offload_work_handler+0x15e [nf_flow_table]
        process_one_work+0x240 [kernel]
        worker_thread+0x50 [kernel]
        kthread+0x109 [kernel]
        ret_from_fork+0x35 [kernel]


nf_flow_offload_add
flow_offload_queue_work
        schedule_work(&nf_flow_offload_work);


tcf_ct_act
	tcf_ct_flow_table_process_conn
		tcf_ct_flow_table_add
			struct flow_offload *flow = flow_offload_alloc
			flow_offload_add
				__set_bit(NF_FLOW_HW, &flow->flags);
				nf_flow_offload_add
					flow_offload_queue_work
						schedule_work(&nf_flow_offload_work);
							flow_offload_work_handler
								flow_offload_work_add
									flow_offload_work_add
										flow_offload_rule_add
											flow_offload_tuple_add
												nf_flow_offload_tuple
													mlx5_tc_ct_block_flow_offload



					nf_flow_offload_tuple
						nf_flow_offload_init
							cls_flow->cookie

flow_offload_work_handler(FLOW_CLS_REPLACE)
	flow_offload_work_add
		flow_offload_rule_add
			flow_offload_tuple_add
			flow_offload_tuple_add

6.494420 15108   15108   kworker/13:1    mlx5_eswitch_add_offloaded_rule
        mlx5_eswitch_add_offloaded_rule
        mlx5_tc_ct_block_flow_offload
        nf_flow_offload_tuple
        flow_offload_work_handler
        process_one_work+0x240 [kernel]
        worker_thread+0x50 [kernel]
        kthread+0x109 [kernel]
        ret_from_fork+0x35 [kernel]


nf_flow_offload_del is called because test_bit(NF_FLOW_TEARDOWN, &flow->flags)
NF_FLOW_TEARDOWN is set because of FIN is received in
	tcf_ct_act()->tcf_ct_flow_table_lookup()->flow_offload_teardown()

[Wed Apr  8 14:27:24 2020] nf_flow_offload_gc_step: expried: 0, dying: 0, teardown: 1

nf_flow_offload_gc_step
	nf_flow_offload_del
		NF_FLOW_HW_DYING
		flow_offload_queue_work(offload)
			schedule_work(&nf_flow_offload_work)

flow_offload_work_handler(FLOW_CLS_DESTROY)
	flow_offload_work_del(FLOW_OFFLOAD_DIR_ORIGINAL)
	flow_offload_work_delFLOW_OFFLOAD_DIR_REPLY)
	set_bit(NF_FLOW_HW_DEAD)

7.037005 8055    8055    kworker/4:1     mlx5_eswitch_del_offloaded_rule
        mlx5_eswitch_del_offloaded_rule+0x1 [mlx5_core]
        mlx5_tc_ct_entry_del_rules+0x29 [mlx5_core]
        mlx5_tc_ct_block_flow_offload
        nf_flow_offload_tuple
        flow_offload_tuple_del
        flow_offload_work_handler
        process_one_work+0x240 [kernel]
        worker_thread+0x50 [kernel]
        kthread+0x109 [kernel]
        ret_from_fork+0x35 [kernel]

[root@dev-r630-04 ~]# trace flow_offload_teardown -K
TIME     PID     TID     COMM            FUNC
8.002926 0       0       swapper/11      flow_offload_teardown
        flow_offload_teardown+0x1 [nf_flow_table]
        tcf_ct_act+0x6ea [act_ct]
        tcf_action_exec+0x82 [kernel]
        fl_classify+0x6d4 [cls_flower]
        tcf_classify_ingress+0x81 [kernel]
        __netif_receive_skb_core+0x441 [kernel]
        __netif_receive_skb_list_core+0x126 [kernel]
        netif_receive_skb_list_internal+0x1f6 [kernel]
        gro_normal_list.part.0+0x1e [kernel]
        napi_complete_done+0x91 [kernel]
        mlx5e_napi_poll+0x1a0 [mlx5_core]
        net_rx_action+0x13b [kernel]
        __softirqentry_text_start+0xf5 [kernel]
        irq_exit+0xdf [kernel]
        do_IRQ+0x5a [kernel]
        ret_from_intr+0x0 [kernel]
        cpuidle_enter_state+0xc2 [kernel]
        cpuidle_enter+0x2e [kernel]
        call_cpuidle+0x23 [kernel]
        do_idle+0x1cd [kernel]
        cpu_startup_entry+0x20 [kernel]
        start_secondary+0x15a [kernel]
        secondary_startup_64+0xa4 [kernel]



[root@dev-r630-04 kernel]# ./mlx5e_tc_flow.py
hostname: dev-r630-04

MLX5E_TC_FLOW_FLAG_INGRESS            1
MLX5E_TC_FLOW_FLAG_ESWITCH            8
MLX5E_TC_FLOW_FLAG_OFFLOADED         20
MLX5E_TC_FLOW_FLAG_CT              1000

MLX5_MATCH_OUTER_HEADERS              1
MLX5_MATCH_MISC_PARAMETERS            2
MLX5_MATCH_MISC_PARAMETERS_2          8

enp4s0f0np0    mlx5e_tc_flow ffff8bf33c2da800, cookie: ffff8bf3162a5000, flags: 1029
chain: 0        dest_chain: b   fdb: 0  dest_ft: 0      ct_state: 0/4
mlx5_flow_spec ffff8bf3162a1818

enp4s0f0np0_1  mlx5e_tc_flow ffff8bf33c2db800, cookie: ffff8bf3162a4800, flags: 1029
chain: 0        dest_chain: a   fdb: 0  dest_ft: 0      ct_state: 0/4
mlx5_flow_spec ffff8bf3162a2818

enp4s0f0np0_1  mlx5e_tc_flow ffff8bf31895cc00, cookie: ffff8bf3162a3800, flags: 29
chain: a        dest_chain: 0   fdb: 0  dest_ft: 0      ct_state: 6/6
mlx5_flow_spec ffff8bf3162a0818

enp4s0f0np0    mlx5e_tc_flow ffff8beaff04b800, cookie: ffff8bf2897bc000, flags: 29
chain: b        dest_chain: 0   fdb: 0  dest_ft: 0      ct_state: 6/6
mlx5_flow_spec ffff8bf2897ba818

[root@dev-r630-04 kernel]# dpd
ct_state(+est+trk),recirc_id(0xa),in_port(enp4s0f0np0_1),eth(src=02:25:d0:14:01:02,dst=b8:59:9f:bb:31:66),eth_type(0x0800),ipv4(proto=6,frag=no), packets:1733099551, bytes:2537255428754, used:0.560s, actions:enp4s0f0np0
ct_state(+est+trk),recirc_id(0xb),in_port(enp4s0f0np0),eth(src=b8:59:9f:bb:31:66,dst=02:25:d0:14:01:02),eth_type(0x0800),ipv4(proto=6,frag=no), packets:29013837, bytes:1916513026, used:0.560s, actions:enp4s0f0np0_1
ct_state(-trk),recirc_id(0),in_port(enp4s0f0np0),eth_type(0x0800),ipv4(proto=6,frag=no), packets:29013837, bytes:1916513026, used:0.560s, actions:ct,recirc(0xb)
ct_state(-trk),recirc_id(0),in_port(enp4s0f0np0_1),eth_type(0x0800),ipv4(proto=6,frag=no), packets:1733099552, bytes:2537255429046, used:0.560s, actions:ct,recirc(0xa)

1. If the rule has ct action, MLX5E_TC_FLOW_FLAG_CT is set.

parse_tc_fdb_actions
	flow_flag_set(flow, CT)

__mlx5_tc_ct_flow_offload() will be called.

The dest ft will be mlx5e_rep_priv.uplink_priv.ct_priv.ct.

19.41299 3798    30693   handler66       __mlx5_tc_ct_flow_offload
        __mlx5_tc_ct_flow_offload+0x1 [mlx5_core]
        mlx5e_tc_offload_fdb_rules+0x6b [mlx5_core]
        mlx5e_tc_add_fdb_flow+0x179 [mlx5_core]
        __mlx5e_add_fdb_flow+0x16a [mlx5_core]
        mlx5e_tc_add_flow+0x170 [mlx5_core]
        mlx5e_configure_flower+0x2c8 [mlx5_core]
        mlx5e_rep_setup_tc_cls_flower+0x39 [mlx5_core]
        mlx5e_rep_setup_tc_cb+0x40 [mlx5_core]
        tc_setup_cb_add+0xdb [kernel]
        fl_hw_replace_filter+0x17a [cls_flower]
        fl_change+0xc0e [cls_flower]
        tc_new_tfilter+0x861 [kernel]
        rtnetlink_rcv_msg+0x369 [kernel]
        netlink_rcv_skb+0x50 [kernel]
        rtnetlink_rcv+0x15 [kernel]
        netlink_unicast+0x1a8 [kernel]
        netlink_sendmsg+0x233 [kernel]
        sock_sendmsg+0x65 [kernel]
        ____sys_sendmsg+0x215 [kernel]
        ___sys_sendmsg+0x81 [kernel]
        __sys_sendmsg+0x5c [kernel]
        __x64_sys_sendmsg+0x1f [kernel]
        do_syscall_64+0x5a [kernel]
        entry_SYSCALL_64_after_hwframe+0x44 [kernel]



mask is 0x5, the match val is 0. That means if it is FIN or RST (PRM page 1965),
don't match, go through slow path.

[root@dev-r630-04 kernel]# ./mlx5_tc_ct_priv.py
hostname: dev-r630-04

=== mlx5e_rep_priv.uplink_priv.ct_priv.ct ===
mlx5_flow_table ffff94e74442dc00

flow table name: ct_priv.ct
flow table id: 6 leve: 1, type: 4
mlx5_flow_table ffff94e74442dc00
mlx5_flow_group ffff94f512f41568	(tcp_flags: 0x5)
fs_fte ffff94f552b22428
       0:  s: 00:00:00:00:00:00 d: 00:00:00:00:00:00 et: 800 ip: 6  sport: 47672 dport:   5001 src_ip:      1.1.3.1 dst_ip:    1.1.1.200 action   4c:
                mlx5_flow_rule ffff94f558a5bc00
                        dest: counter_id: 801005
                mlx5_flow_rule ffff94f558a5a200
                        dest: ft: ffff94e744428400
fs_fte ffff94f552b250a8
       1:  s: 00:00:00:00:00:00 d: 00:00:00:00:00:00 et: 800 ip: 6  sport:  5001 dport:  47672 src_ip:    1.1.1.200 dst_ip:      1.1.3.1 action   4c:
                mlx5_flow_rule ffff94f558a5ba00
                        dest: counter_id: 801006
                mlx5_flow_rule ffff94f558a59400
                        dest: ft: ffff94e744428400




[Sat Apr 11 18:05:44 2020] mlx5e_configure_flower                  : enp4s0f0np0_1       : is called
[Sat Apr 11 18:05:44 2020] __parse_cls_flower                      : enp4s0f0np0_1       : is called, 800
[Sat Apr 11 18:05:44 2020] __mlx5_tc_ct_flow_offload               : enp4s0f0np0_1       : is called

[Sat Apr 11 18:05:44 2020] mlx5e_configure_flower                  : enp4s0f0np0_1       : is called
[Sat Apr 11 18:05:44 2020] __parse_cls_flower                      : enp4s0f0np0_1       : is called, 800

[Sat Apr 11 18:05:44 2020] mlx5e_configure_flower                  : enp4s0f0np0         : is called
[Sat Apr 11 18:05:44 2020] __parse_cls_flower                      : enp4s0f0np0         : is called, 800
[Sat Apr 11 18:05:44 2020] __mlx5_tc_ct_flow_offload               : enp4s0f0np0         : is called

[Sat Apr 11 18:05:44 2020] mlx5e_configure_flower                  : enp4s0f0np0         : is called
[Sat Apr 11 18:05:44 2020] __parse_cls_flower                      : enp4s0f0np0         : is called, 800

[Sat Apr 11 18:05:44 2020] mlx5e_configure_flower                  : enp4s0f0np0_1       : is called
[Sat Apr 11 18:05:44 2020] __parse_cls_flower                      : enp4s0f0np0_1       : is called, 800

[Sat Apr 11 18:05:44 2020] mlx5_tc_ct_entry_add_rules              : enp4s0f0np0         : is called
[Sat Apr 11 18:05:44 2020] mlx5_tc_ct_entry_add_rules              : enp4s0f0np0         : is called

===netlink===

dpif_netlink_port_add
	dpif_netlink_rtnl_port_create_and_add
		dpif_netlink_refresh_channels for restart
		dpif_netlink_port_add__
			create_nl_sock
				nl_sock_create (NETLINK_GENERIC)

===upcall===

epoll_ctl (handler->epoll_fd, EPOLL_CTL_ADD, nl_sock_fd(sock), &event))

udpif_upcall_handler
	recv_upcalls
		upcall_receive
			classify_upcall
				USER_ACTION_COOKIE_SFLOW
				SFLOW_UPCALL
		process_upcall
			dpif_sflow_received

===psample===

        netlink_broadcast_filtered
netlink_broadcast
        psample_sample_packet
        tcf_sample_act
        tcf_action_exec
        mall_classify+0x3e [cls_matchall]
        tcf_classify_ingress+0x81 [kernel]
        __netif_receive_skb_core+0x441 [kernel]
        __netif_receive_skb_list_core+0x126 [kernel]
        netif_receive_skb_list_internal+0x1f6 [kernel]
        gro_normal_list.part.0+0x1e [kernel]
        napi_complete_done+0x91 [kernel]
        mlx5e_napi_poll+0x1a0 [mlx5_core]
        net_rx_action+0x13b [kernel]
        __softirqentry_text_start+0xf5 [kernel]
        irq_exit+0xdf [kernel]
        do_IRQ+0x5a [kernel]
        ret_from_intr+0x0 [kernel]
        cpuidle_enter_state+0xc2 [kernel]
        cpuidle_enter+0x2e [kernel]
        call_cpuidle+0x23 [kernel]
        do_idle+0x1cd [kernel]
        cpu_startup_entry+0x20 [kernel]
        start_secondary+0x15a [kernel]
        secondary_startup_64+0xa4 [kernel]

===datapath===

do_execute_actions
	OVS_ACTION_ATTR_SAMPLE
	sample
		clone_execute
			do_execute_actions
				OVS_ACTION_ATTR_USERSPACE
				output_userspace
					ovs_dp_upcall
						queue_userspace_packet

===tc===

parse_sample

===ovs===

compose_sflow_action
fix_sflow_action

nl_sock_create

pid is set by getsockname()

in kernel, it is set by netlink_bind netlink_autobind


dpif_netlink_operate
try_send_to_netdev
parse_flow_put
flow_api->flow_put

netdev_tc_flow_put
tc_replace_flower
nl_msg_put_flower_options
nl_msg_put_flower_acts
nl_msg_put_act_mirred

===question===

1. What is FW level support 
mlx5_tc_ct_init_check_support()
	err_msg = "firmware level support is missing";

2. a.	Table 1. Do match, set group id (set register c0), jump to sample.
It seems many registers are used by CT, CHAIN_TO_REG for c0

3. what is vport 0

4. what is losing register

5. What is FLOW_SAMPLE_OBJ

6. Which FW should be used?

7. What's the API to receive multicast netlink message?
resolved

8. How do I know which flow table is hit or missed? Any FW tools?

9. What is register loopback?

10. sFlow in HW will initiate miss to SW? How to do?

11. what is vport0? What is vport 0 NIC_RX.

12. What is losing registers issue?

13. Sample is a new action type?

enum mlx5_flow_destination_type {
        MLX5_FLOW_DESTINATION_TYPE_VPORT        = 0x0,
        MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE   = 0x1,
        MLX5_FLOW_DESTINATION_TYPE_TIR          = 0x2,

        MLX5_FLOW_DESTINATION_TYPE_PORT         = 0x99,
        MLX5_FLOW_DESTINATION_TYPE_COUNTER      = 0x100,
        MLX5_FLOW_DESTINATION_TYPE_FLOW_TABLE_NUM = 0x101,
};

14. Who send up-call to userspace, mlx5_core or act_sample

15. I need Paul or Oz talk about the CT MT implementation.

16. What is shadow table?

17. What is miss? If we specify a table, where should the packet go

18. Mirror is supported due to FW limitation?
